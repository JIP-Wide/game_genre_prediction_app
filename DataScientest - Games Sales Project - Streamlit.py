import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import plotly.graph_objects as go
from matplotlib.ticker import FuncFormatter
import seaborn as sns
import xgboost as xgb
from wordcloud import WordCloud, STOPWORDS
from PIL import Image
import os
import re
import nltk
from nltk.corpus import stopwords
from string import punctuation
from nltk.stem import WordNetLemmatizer
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from sklearn.preprocessing import PolynomialFeatures
import json
import zipfile

# Set the page layout
st.set_page_config(page_title = "DataScientest Project - Games Sales", layout = "wide")

# ðŸ“¦ Ensure all  NLTK packages are available
nltk_resources = ["stopwords", "punkt", "wordnet", "omw-1.4"]
for res in nltk_resources:
    try:
        nltk.data.find(f"corpora/{res}")
    except LookupError:
        with st.spinner(f"Lade NLTK-Daten: {res}"):
            nltk.download(res)

# Add an invisible anchor at the top of the page for the "Go to top" button
st.markdown('<a id="top"></a>', unsafe_allow_html=True)

# Add custom HTML and JavaScript for the floating button
st.markdown("""
    <style>
    #scroll-to-top {
        position: fixed;
        bottom: 40px;
        right: 20px;
        background-color: darkgrey;
        color: white;
        border: none;
        padding: 2px 16px;
        border-radius: 5%;
        font-size: 28px;
        cursor: pointer;
        z-index: 1000;
    }
    </style>
    
    <a href="#top">
        <button id="scroll-to-top" title="Go to top">^</button>
    </a>
    """, unsafe_allow_html=True)

# Sidebar for navigation
st.sidebar.title("Navigation")
sections = st.sidebar.radio("Select a Section:", ["Project Overview",
                                                  "Data Preprocessing and Modeling",
                                                  "Game Genre Prediction Demo",
                                                  "Project Conclusion",
                                                  "Credits"])

if sections == "Project Overview":
    st.markdown("<div style='margin-top: 10px;'></div>", unsafe_allow_html = True)  # Adjust spacing before header for "Go to top" button

    # Load datasets
    df_reviews = pd.read_csv('all_user_reviews_data_normalized_105000.csv')
    df_games = pd.read_csv('complete_records_3280.csv')

    st.header("Project Overview")
    st.subheader("Introduction") # 1. 

    # Create a layout with columns to center the content
    layout_mario = st.columns([3.2, 0.8])  # Adjust the column widths for centering

    with layout_mario[0]:
        st.write("""
                <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
                 
                This demo highlights the prediction of video game genres â€” a core component of our broader project on video game analysis. To accomplish this goal, 
                I relied on a significant amount of data scraped from the Metacritic website, including Metacritic scores, user reviews, critic reviews, platform counts,
                game difficulty, game completion and play time. 
                 
                Based on nearly 100,000 user reviews, it demonstrates how text mining and machine learning can be leveraged to deliver personalized game recommendations
                and enable targeted advertising strategies for different genres based on usersâ€™ preferences and experiences.
                 
                </div>
                """,
                unsafe_allow_html=True)
                
    with layout_mario[1]:
        # Display the GIF in the center column
        st.image('Visualisations/Streamlit Pictures/mario_tube.gif', width = 200)

    st.subheader("Data Overview")
    st.write("""
            <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
             
            The data used contains of two main data sets, which were generated by webscraping with Silenium from the metacritic.com webpage.
            
            The first dataset contains user reviews.
            This dataset contained information about game titles, platforms, global user scores, individual user scores, usernames, and user reviews.
            With over 105,000 entries, this dataset formed the foundation for the analysis.
            The initial exploration showed six features, including text-based user reviews that varied significantly in terms of content and sentiment.
             
            </div>
            """,
            unsafe_allow_html=True)
    
    st.markdown("<div style='margin-top: 10px;'></div>", unsafe_allow_html = True)
    
    # st.write("**User Reviews Dataframe**")
    with st.expander("User Reviews Dataframe"):
        layout_df_info_1 = st.columns([12, 0.5, 6.2])
        with layout_df_info_1[0]:
            st.dataframe(df_reviews, height = 250)

        with layout_df_info_1[2]:
            st.image("Visualisations/df_reviews_info.png") #, use_container_width = False)

    st.markdown("<div style='margin-top: 10px;'></div>", unsafe_allow_html = True)

    st.write("""
            <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
             
            The second dataset dataset containins information about the games themselves.
            This dataset featured details like the platform, publisher, year of release, game genre, regional and total sales as well as different game metrics,
            such as difficulty, play time and completion. The dataset consisted of 3,280 games, with 36 different features.
             
            </div>
            """,
            unsafe_allow_html=True)

    st.markdown("<div style='margin-top: 10px;'></div>", unsafe_allow_html = True)

    with st.expander("Games Data Dataframe"):
        layout_df_info_2 = st.columns([12, 0.5, 6.2])
        with layout_df_info_2[0]:
            st.dataframe(df_games, height = 600) 

        with layout_df_info_2[2]:
            st.image("Visualisations/df_games_info.png", use_container_width = False) 

if sections == "Data Preprocessing and Modeling":
    st.markdown("<div style='margin-top: 10px;'></div>", unsafe_allow_html = True)  # Adjust spacing before header for "Go to top" button

    # Unzip and load datasets
    zip_path = "all_user_reviews_translated_preprocessed_with_games_data_streamlit.zip"  # path in your repo
    extract_dir = "temp"  # or another temp subfolder

    # Unzip only if not already done
    if not os.path.exists(extract_dir):
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(extract_dir)

    df_games_reviews = pd.read_csv(os.path.join(extract_dir, 'all_user_reviews_translated_preprocessed_with_games_data_streamlit.csv'))
    df_games_reviews_X_test = pd.read_csv('all_user_reviews_with_games_data_X_y_test_streamlit.csv')

    game_genre_tabs = st.tabs(["Data Preprocessing",
                               "Modeling",
                               "Evaluation"])

    ################### DATA PREPROCESSING ###################

    with game_genre_tabs[0]:
        st.subheader("User Review Preprocessing") # 2.1 

        st.write("#### Translation of non-English reviews to English using GoogleTranslator.")

        st.write("""
                <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
                 
                In the given data not all reviews were in English. Some reviews were written in Italian and Russian, indicating that our dataset had an 
                international flavour, which posed a challenge for consistency. To ensure uniformity, we used the Google Translator API via the deep-translator 
                library to translate all non-English reviews into English. This was crucial for maintaining a consistent language across all reviews, allowing 
                us to analyze and process the text without language barriers.

                Some reviews could not be translated successfully, which led to a decision to mark and eventually drop these entries.
                The reviews that remained were entirely in English, and those that couldn't be reliably translated or were NaN were removed.
                Fortunately, this only accounted for about 1.39% of the total dataset, meaning that data loss was minimal.
                Removing all these values ensured that the dataset was complete, consistent, and ready for the next stages of processing.
                 
                </div>
                """,
                unsafe_allow_html=True)

        layout_waterfall = st.columns([2, 10, 2])

        with layout_waterfall[1]:
            # Data for the waterfall chart
            steps = [
                "Total Reviews",
                "English Reviews",
                "Translated Reviews",
                "Untranslated Reviews",
                "NaN Reviews Dropped",
            ]
            values = [105423, -90613, -(14810 - 1854), -(1854 - 73), -73]

            # Create the waterfall chart
            fig = go.Figure(
                go.Waterfall(
                    name = "Reviews",
                    orientation = "v",
                    measure = ["absolute"] + ["relative"] * (len(values) - 1),
                    x = steps,
                    y = values,
                    text = [f"{-val:,}" if val != 105423 else f"{val:,}" for val in values],
                    textposition = "auto",
                    increasing=dict(marker = dict(color = 'rgb(0, 153, 204)')),  # Assign custom colors
                    decreasing=dict(marker = dict(color = 'rgb(0, 153, 204)')),  # Ensure same color scheme
                    totals=dict(marker = dict(color = 'rgb(0, 153, 204)'))  # Custom color for totals
                )
            )
            
            # Customize layout
            fig.update_layout(
                title = "User Review Translation Steps",
                xaxis_title = "Translation Steps",
                yaxis_title = "Number of Reviews",
                yaxis = dict(tickformat = ","),
                barmode = "stack",
                showlegend = False,
                template = "simple_white",
            )

            st.plotly_chart(fig)

            with st.expander("Waterfall Chart Description"):
                st.write("""
                        The waterfall chart shows the translation process for user reviews:
                        - Total reviews in the raw dataset: **105,423**.
                        - English reviews: **90,613**, non-English reviews: **14,810**.
                        - After translation, only **1,854** remained untranslated.
                        - Within untreanslated reviews **73** were dropped due to being **NaN**.
                        """)
            st.markdown('<style>[data-testid="stMarkdownContainer"] ul{padding-left:40px;}</style>', unsafe_allow_html=True)

        # ======================================================
        st.markdown("<div style='margin-top: 20px;'></div>", unsafe_allow_html = True)
        st.write("#### Removing punctuation, stopwords and lemmatizing review text")
        st.write("""
                <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
                 
                To make the textual reviews suitable for modeling, we carried out extensive preprocessing.
                This step involved transforming the raw text data into a more structured form that could be used effectively by machine learning models.
                
                **Normalization:**  
                We removed punctuation, numbers, and any non-alphanumeric symbols, which often do not contribute meaningful information for sentiment 
                or classification tasks. Additionally, all text was converted to lowercase to ensure uniformity.

                **Stopword Removal and Lemmatization:**  
                We used the NLTK library to remove common stopwords (e.g. **"the"**, **"is"**, and **"and"**), which do not carry much significance for our analysis. 
                Specific gaming-related words, such as **"game"** or **"games"**, were also added to the list of stopwords to prevent bias. 
                Each word was then lemmatized to reduce it to its base form, ensuring that variations like **"playing"** and **"played"** were treated as the same word.
                
                </div>
                """,
                unsafe_allow_html=True)
        
        st.write("#### Word Clouds by Genre")
        st.write("""
                <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
                
                The effects achieved by **normalization**, **stop words removal** and **lemmatization**
                are visualised in corresponding the wordclouds (raw and preprocessed) for each genre below.

                The raw text reflects generic and less contextualized sentiments about games.
                The dominance of words like **"game"** and **"play"** shows that users focus on the game itself 
                but do not offer significant insights into specific features or experiences.

                The preprocessing steps have effectively cleaned and refined the dataset, 
                providing a clearer understanding of what aspects of games players focus on.
                The processed reviews offer actionable insights into player preferences 
                (e.g., **storylines**, **character development**, **gameplay mechanics**) that are critical 
                for game developers or marketers to improve and promote action games.
                 
                </div>
                """,
                unsafe_allow_html=True)

        # Word cloud folder
        wordclouds_folder = "Visualisations/Word Clouds"

        genres = list(np.sort(df_games_reviews['genre'].unique())) + ["Show All"]
        selected_genre = st.radio("", genres, horizontal = True)  # No label for a cleaner look

        # Define layout with one sidebar-like column and two main columns
        layout_wordclouds = st.columns([4, 4])  # Left column is narrower; adjust widths as needed

        # Define a function to load comments from a `.txt` file
        def load_comment(genre):
            try:
                with open(os.path.join(wordclouds_folder, f"{genre}.txt"), "r") as file:
                    comment = file.read()
                return comment
            except FileNotFoundError:
                return "No comments available for this genre."

        # Display word clouds for row and preprocessed reviews per genre
        if selected_genre == "Show All":
            for genre in df_games_reviews['genre'].unique():
                with layout_wordclouds[0]:
                    st.markdown(f"<h3 style='text-align: center;'>{genre} (Raw)</h3>", unsafe_allow_html = True)
                    st.image(os.path.join(wordclouds_folder, f"{genre}_review_text.png"), use_container_width = True)
                with layout_wordclouds[1]:
                    st.markdown(f"<h3 style='text-align: center;'>{genre} (Preprocessed)</h3>", unsafe_allow_html = True)
                    st.image(os.path.join(wordclouds_folder, f"{genre}_review_preprocessed.png"), use_container_width = True)
        else:
            with layout_wordclouds[0]:
                st.markdown(f"<h3 style='text-align: center;'>{selected_genre} (Raw)</h3>", unsafe_allow_html = True)
                st.image(os.path.join(wordclouds_folder, f"{selected_genre}_review_text.png"), use_container_width = True)
                # Load the comment for the selected genre
                with st.expander(f"Word cloud raw explanation for {selected_genre}"):
                    comment = load_comment(f"{selected_genre}_review_text")
                    st.write(f"""
                            <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
                             
                            {comment}
                             
                            </div>
                            """,
                            unsafe_allow_html=True)
            with layout_wordclouds[1]:
                st.markdown(f"<h3 style='text-align: center;'>{selected_genre} (Preprocessed)</h3>", unsafe_allow_html = True)
                st.image(os.path.join(wordclouds_folder, f"{selected_genre}_review_preprocessed.png"), use_container_width = True)
                with st.expander(f"Word cloud preprocessed explanation for {selected_genre}"):
                    comment = load_comment(f"{selected_genre}_review_preprocessed")
                    st.write(f"""
                            <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
                             
                            {comment}
                             
                            </div>
                            """,
                            unsafe_allow_html=True)

        # ======================================================
        st.subheader("Games Data Preprocessing") # 2.2 
        st.write("""
                <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
                 
                To make the dataset manageable and focused, we removed columns that were unlikely to impact the user review analysis. 
                For instance, details about the developer and the number of platforms were removed since user reviews were generally 
                tied to specific platforms and developers were less likely to affect user perception directly. Instead, columns like 
                "publisher" were kept, as publisher reputation could influence user expectations and reviews.

                #### Reducing Feature Columns for Game Metrics by Combining them

                We simplified several features from games metrics by aggregating related columns. Specifically, we calculated weighted 
                average values for "Play Time", "Game Difficulty", and "Game Completion" metrics, replacing the detailed but numerous individual 
                columns with simpler, aggregated metrics. This was done to reduce dimensionality and make the dataset more efficient 
                for modeling without losing meaningful information. This conversion allowed us to reduce **twenty columns** of game metrics to just **three columns**.
                
                </div>
                """,
                unsafe_allow_html=True)
        with st.expander("Example for calculation of play time"):
            st.image("Visualisations/play_time_calculation_example.png", use_container_width = True)
        with st.expander("Example for calculation of game difficulty"):
            st.image("Visualisations/game_difficulty_calculation_example.png", use_container_width = True)
        with st.expander("Example for calculation of game completion"):
            st.image("Visualisations/game_completion_calculation_example.png", use_container_width = True)

        st.write("#### Removing Redundant Sales Features with high Correlation")
        st.write("""
                <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
                 
                After merging the user reviews and game metadata, the next step was to thoroughly clean the resulting dataset. 
                This involved simplifying the dataset by removing redundant or highly correlated features to reduce dimensionality while preserving predictive power.
                 
                </div>
                """,
                unsafe_allow_html=True)
        with st.expander("Heatmap of raw correlation matrix"):
            st.image("Visualisations/correlation_matrix_raw.png", use_container_width = True)
        
        st.write("""
                <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
                 
                Using a heatmap of the correlation matrix for all numeric variables, we identified several highly correlated features. 
                So, **NA_Sales**, **EU_Sales**, and **Other_Sales** were all strongly correlated with **Total_Sales** (correlation > 0.8). 
                Since these individual sales metrics add little unique information beyond what Total_Sales already provided, 
                we decided to retain only **Total_Sales** and **JP_Sales**, dropping the rest.
                 
                </div>
                """,
                unsafe_allow_html=True)
        with st.expander("Heatmap of cleaned correlation matrix"):
                st.image("Visualisations/correlation_matrix_cleaned.png", use_container_width = True)

        st.write("""
                <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
                
                Finally, we removed columns like **game_title**, **username**, and **review_text**, 
                which were not needed for modeling, as they did not provide additional predictive value.
                 
                </div>
                """,
                unsafe_allow_html=True)

    ####################### MODELLING ########################

    with game_genre_tabs[1]: # 2
        st.header("Modeling Process") # 3. 

        st.subheader("Feature Preprocessing") # 3.1 
        
        st.write("""
                 <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>

                The goal of transforming and processing the dataset was to ensure we had a high-quality, structured input for modeling, 
                especially given the challenges of data size and class imbalance. Here, we document the step-by-step approach to prepare 
                the data effectively for training and evaluation.
                    
                </div>
                """,
                unsafe_allow_html=True)

        layout_balance = st.columns([2, 10, 2])

        with layout_balance[1]:
            fig, ax = plt.subplots(figsize = (20, 8))
            ax = sns.countplot(x = 'genre', data = df_games_reviews, order = df_games_reviews.genre.value_counts().index, color = 'lightgreen')
            ax.bar_label(ax.containers[0], label_type = 'edge', padding = 1, fontsize = 20, fmt = lambda x: f'{x/df_games_reviews.genre.count()*100:0.1f}%')
            ax.margins(y = 0.1)

            plt.xticks(rotation = 45, fontsize = 20)
            plt.yticks(fontsize = 20)
            plt.xlabel('Game Genre', fontsize = 20)
            plt.ylabel('Count', fontsize = 20)
            plt.title('Number of User Reviews per Genre (balance check)', fontsize = 20)
            st.pyplot(fig)

            with st.expander('**Genre/Review-Imbalance** Chart Description'):
                st.write("""
                        <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
                        
                        The distribution of user reviews across game genres was highly imbalanced. The genres **'Action' (29.5%)** and **'Shooter' (18.1%)** 
                        and **'Role-Playing' (14,1%)** were significantly overrepresented compared to others like **'Puzzle' (0.9%)** and **'Simulation' 
                        (2,2%)**. This imbalance needed to be addressed before proceeding to model training to ensure fair learning across all genres.st.write
                        
                        </div>
                        """,
                        unsafe_allow_html=True)
                
        st.write("""
                <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
                 
                As the dataset is very big for the given facilities and the classes in the target variable are imbalanced, we applied a step-by-step approach.
                It means, instead of putting all the transformers (scaler, encoder, vectorizer, resampler) and model into one pipeline, we tackled each individual
                step separately, which allowed better focusing on details and debugging in case of any issues.
                 
                </div>
                """,
                unsafe_allow_html=True)

        with st.expander('Step-by-Step Approach for **Feature Preprocessing** and **Model Preparation**'):
            st.write("""
                    <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
                        
                    1.  Separate the dataset into **Features (X)** and **Target (y)**
                    2.  Split the data into **Train/Test Sets** to ensure no test set information leaks into training or oversampling
                    3.  Preprocess all categorical features in the resampled Training Set (**OneHotEncoder** and **TfidfVectorizer**) since **SMOTETomek** 
                    (or SMOTE) expects all input features to be numeric
                    4.  Oversample the Training Set by applying **SMOTETomek** to **X_train** and **y_train** only in order to avoid creation of 
                    synthetic samples derived from test data, leading to over-optimistic evaluation metrics
                    5.  Preprocess all numeric features in the resampled Training Set (**StandardScaler**, **MinMaxScaler**, **RobustScaler**), ensuring that no 
                    information leaks from the Test Set and any unseen categories in the test set are handled gracefully (for OneHotEncoder)
                    6.  Create Smaller **Subset** for accelerated modelling
                    7.  Use the Oversampled Sample Data for **Model Training**:  
                        - Use **X_train_sample** and **y_train_sample** for training  
                        - Keep **X_test** and **y_test** untouched for evaluation
                    
                    </div>
                    """,
                    unsafe_allow_html=True)
        st.markdown('<style>[data-testid="stMarkdownContainer"] ul{padding-left:40px;}</style>', unsafe_allow_html = True)

        st.write("#### Separation of Dataset into Features (X) and Target (y) and splitting with train_test_split")
        st.write("""
                <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
                 
                In our first attempts, we encountered problems with the column names **'year'**, **'difficulty'** and **'completion'** 
                after using TfidfVectoriser, as new columns with the same names were created from the tokens. 
                To avoid this problem, the original columns are renamed to **'release_year'**, **'game_difficulty'** and **'game_completion'** from the outset.

                The target variable **genre** was mapped to numeric values and the original genre column was dropped. 
                The target variable (**y**) was set as the newly created **genre_num** column, while the remaining columns formed the features (**X**).
                
                </div>
                """,
                unsafe_allow_html=True)
        
        with st.expander('Calculation of **test_size** to Optimize the Dataset Size'):
            st.write("""
                    <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>

                    Given the large size of the dataset, we experienced computational challenges when attempting to process all data through the preprocessing 
                    and modeling steps. To address these issues, we opted to split the dataset to create a manageable workflow.
                    The primary goal was to have around **15,000** samples each in the preprocessed **X_test** and in the resampled **X_train** datasets, 
                    totaling approximately **30,000** samples. To achieve this, we began by splitting **X** into **X_train** and **X_test**, ensuring that **X_test** 
                    contained **15,000** samples by calculating of the **test_size** for the first **train_test_split**:
                    
                    </div>
                    """,
                    unsafe_allow_html=True)
            st.markdown(r"""
                        $$
                        \text{round}\left(\frac{15000}{X.shape[0]}, 3\right) = 0.148
                        $$
                        """)
            st.write("""
                    After splitting X with calculated test_size we got the following result:  
                    - X_train: 86,564 samples  
                    - X_test: 15,037 samples  
                    """)
            st.markdown('<style>[data-testid="stMarkdownContainer"] ul{padding-left:40px;}</style>', unsafe_allow_html=True)
            st.write("""
                    <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
                     
                    At every stage of splitting, we used the stratify parameter to maintain consistent class distribution across all splits.

                    Since the original dataset contained 101,601 samples, the initial **X_train** contained around **86,500** samples, which was still 
                    too large for efficient processing. After applying **SMOTETomek** for oversampling, **X_train** would have expanded to approximately 
                    **306,000** samples, far exceeding our computational capabilities.

                    To mitigate this, we implemented an intermediate step where we halved the **X_train** dataset before proceeding with preprocessing 
                    and oversampling. This step produced **43,282** samples in **X_train_reduced**. Subsequently, the resampled **X_train_reduced** was 
                    further split so that it contained around **15,000** samples, similar to **X_test**. For this we calculated the **test_size** for the
                    last **train_test_split** as follows:
                     
                    </div>
                    """,
                    unsafe_allow_html=True)
            st.markdown(r"""
                        $$
                        \text{round}\left(1 - \frac{15000}{X_{\text{train, resampled, preprocessed}}.shape[0]}, 3\right) = 0.902
                        $$
                        """)
            st.markdown("<div style='margin-top: 10px;'></div>", unsafe_allow_html = True)

        # ======================================================
        st.write("#### Preprocess Categorical and Textual Features")
        st.write("""
                <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
                 
                To prepare the categorical features (**"platform"** and **"publisher"**) and textual feature (**"review_preprocessed"**) for modeling, we employed the 
                following preprocessing techniques:

                - **OneHotEncoder**
                This was used for categorical features with parameters **drop = 'first'** to prevent multicollinearity and **handle_unknown = 'ignore'** to manage 
                unseen categories during testing.

                - **TfidfVectorizer**
                Applied to the text reviews to convert them into numerical form while preserving the importance of frequently used terms. The original 
                text reviews were transformed into a high-dimensional vector representation, resulting in thousands of new features. Specifically, the 
                vectorizer used the top 5000 most relevant terms (words or tokens) from the preprocessed text. :red[**Each term contributed one feature, resulting 
                in a total of 5000 new columns added to the dataset. This dramatically increased the dimensionality of the data.**]
                 
                </div>
                """,
                unsafe_allow_html=True)

        st.write("#### Preprocess Numerical Features")
        st.write("""
                <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
                 
                To preprocess the numerical features, we first analyzed their distributions using **Q-Q plots**, **boxplots** and **histograms** to determine which 
                scaling approach would be most suitable. Each numeric feature in the dataset has a unique distribution, which makes it essential to tailor 
                the scaling method accordingly. For instance, features like **"critic_review_count"** follow a roughly **normal distribution**, while others such as 
                **"user_review_count"** have a highly **skewed distribution** with **extreme outliers**. Applying the same scaler to all features could result in inappropriate 
                transformations, which might lead to incorrect model assumptions, skewed feature values, or even negative impacts on model performance.
                
                Based on our analysis, we chose different scalers for each group of numerical features:
                    - **Standard Scaler** for features with approximately normal distributions
                    - **MinMax Scaler** for features with known, bounded ranges
                    - **Robust Scaler** for features with possible outliers

                </div>
                """,
                unsafe_allow_html=True)
        st.markdown('<style>[data-testid="stMarkdownContainer"] ul{padding-left:40px;}</style>', unsafe_allow_html=True)

        st.markdown('<style>div[class*="stRadio"] > label > div[data-testid="stMarkdownContainer"] > p {text-align: center; font-size: 18px;}</style>', unsafe_allow_html=True)
        
        # Q-Q Plot folder:
        qq_plot_folder = "Visualisations/Q-Q Plots (normality)"
        boxplot_folder = "Visualisations/Boxplots"
        histogramm_folder = "Visualisations/Histogramms"

        num_features_sc = df_games_reviews[['critic_review_count', 'game_difficulty', 'game_completion']].columns.tolist()
        num_features_minmax = df_games_reviews[['global_user_score', 'user_score', 'release_year', 'metascore', 'play_time']].columns.tolist()
        num_features_rs = df_games_reviews[['jp_sales', 'total_sales', 'avg_rank', 'user_review_count']].columns.tolist()

        layout_num_features = st.columns([2.32, 3.5, 3.5, 3.5])
        with layout_num_features[0]:
            st.markdown(f"<h4 style='text-align: left;'>Scaler/Features</h4>", unsafe_allow_html = True)
            st.markdown("<div style='margin-top: 30px;'></div>", unsafe_allow_html = True)
            selected_num_feature_sc = st.radio("**Standard Scaler**", num_features_sc, horizontal = False)

            st.markdown("<div style='margin-top: 150px;'></div>", unsafe_allow_html = True)
            selected_num_feature_minmax = st.radio("**MinMax Scaler**", num_features_minmax, horizontal = False)

            st.markdown("<div style='margin-top: 120px;'></div>", unsafe_allow_html = True)
            selected_num_feature_rs = st.radio("**Robust Scaler**", num_features_rs, horizontal = False)

        with layout_num_features[1]:
            st.markdown(f"<h4 style='text-align: center;'>Q-Q Plot (normality)</h4>", unsafe_allow_html = True)
            st.image(os.path.join(qq_plot_folder, f"Q-Q Plot (normality) {selected_num_feature_sc}.png"), use_container_width = True)
            st.image(os.path.join(qq_plot_folder, f"Q-Q Plot (normality) {selected_num_feature_minmax}.png"), use_container_width = True)
            st.image(os.path.join(qq_plot_folder, f"Q-Q Plot (normality) {selected_num_feature_rs}.png"), use_container_width = True)

        with layout_num_features[2]:
            st.markdown(f"<h4 style='text-align: center;'>Boxplot</h4>", unsafe_allow_html = True)
            st.image(os.path.join(boxplot_folder, f"Boxplot {selected_num_feature_sc}.png"), use_container_width = True)
            st.image(os.path.join(boxplot_folder, f"Boxplot {selected_num_feature_minmax}.png"), use_container_width = True)
            st.image(os.path.join(boxplot_folder, f"Boxplot {selected_num_feature_rs}.png"), use_container_width = True)

        with layout_num_features[3]:
            st.markdown(f"<h4 style='text-align: center;'>Histogramm</h4>", unsafe_allow_html = True)
            st.image(os.path.join(histogramm_folder, f"Histogramm {selected_num_feature_sc}.png"), use_container_width = True)
            st.image(os.path.join(histogramm_folder, f"Histogramm {selected_num_feature_minmax}.png"), use_container_width = True)
            st.image(os.path.join(histogramm_folder, f"Histogramm {selected_num_feature_rs}.png"), use_container_width = True)

        st.markdown("<div style='margin-top: 10px;'></div>", unsafe_allow_html = True)

        # ======================================================
        st.subheader("Choosing a Classification Model") # 3.2 
        st.write("""
                <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
                
                The genre prediction task can be framed as a **multiclass classification problem**. Given the large and imbalanced nature of the dataset, 
                it was essential to consider models that fulfill the following aspects:  
                    - **Handle Imbalance Well**  
                Despite using SMOTETomek to balance the classes, it's common to still face imbalances or skewed performance on certain classes.  
                    - **Scalability and Efficiency**  
                Since the dataset was large even after downsampling, models that could efficiently handle large feature spaces and provide scalability were essential.  
                    - **Balancing Interpretability and Complexity**  
                Striking a balance between model interpretability and complexity was essential to ensure both actionable insights and strong predictive performance.
                
                </div>
                """,
                unsafe_allow_html=True)
        st.markdown('<style>[data-testid="stMarkdownContainer"] ul{padding-left:40px;}</style>', unsafe_allow_html = True)

        st.write("""
                <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
                 
                **XGBoost** was chosen as the preferred model due to its inherent ability to handle imbalanced datasets effectively, its support for parallelism, 
                and its efficiency in processing large datasets using GPU acceleration. As a Gradient Boosting method, **XGBoost** offered a compelling balance of predictive performance, 
                scalability, and flexibility, making it ideal for achieving optimal prediction accuracy.
                 
                </div>
                """,
                unsafe_allow_html=True)
        
        st.markdown('<style>div[class*="stRadio"] > label > div[data-testid="stMarkdownContainer"] > p {text-align: center; font-size: 18px;}</style>', unsafe_allow_html=True)
        
        # ======================================================
        st.subheader("Hyperparameter Tuning") # 3.3 
        st.write("""
                <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
                 
                After selecting **XGBoost** as our preferred model, the next step was to fine-tune its hyperparameters to achieve optimal performance for the genre 
                classification task. Hyperparameter tuning helps to adjust the parameters that control the learning process of the model, which can lead to improved 
                performance by balancing bias and variance.

                Key goals of hyperparameter tuning include:
                -	Maximizing **accuracy** and **F1 score**
                -	Finding the balance between **bias** and **variance** to ensure that the model generalizes well.
                -	Optimizing the **speed of training** and **predictive accuracy**.
                 
                </div>
                """,
                unsafe_allow_html=True)
        st.markdown('<style>[data-testid="stMarkdownContainer"] ul{padding-left:40px;}</style>', unsafe_allow_html = True)

        st.write("""
                <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
                 
                For this task, we chose **Optuna**, a powerful and efficient hyperparameter optimization framework, because of the following reasons:
                - **Automated Search**  
                    Optuna performs automated searches over a large space of hyperparameters.
                - **Adaptive Sampling**  
                    It uses advanced techniques like Tree-structured Parzen Estimator (TPE), which adaptively narrows down the hyperparameter search based on prior trials.
                - **Efficiency**  
                    Optuna is computationally efficient compared to traditional methods like grid search or random search, which can be extremely time-consuming given the 
                    large dataset size and complex model.
                 
                </div>
                """,
                unsafe_allow_html=True)
        st.markdown('<style>[data-testid="stMarkdownContainer"] ul{padding-left:40px;}</style>', unsafe_allow_html = True)

        st.write("For tuning **XGBoost**, we focused on several key hyperparameters that are most likely to influence the model's performance.")
        with st.expander("Hyperparameter tuning process"):
            st.write("""
                    <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
                     
                    1.  **Define the Objective Function**
                        The objective function is used by Optuna to search for the best combination of hyperparameters. We defined a parameter space that included various 
                        hyperparameters of the **XGBoost** model, such as **max_depth**, **learning_rate**, **subsample**, and more.

                    2.  **Hyperparameter Space**
                        The parameters were defined using Optuna's suggest methods:
                    - **"max_depth":** The maximum depth of the trees, suggested as an **integer between 4 and 10**
                    - **"learning_rate":** The learning rate, suggested as a **float between 0.01 and 0.3 on a log scale**
                    - **"subsample":** The fraction of samples to be used for each tree, suggested as a **float between 0.5 and 1.0**
                    - **"colsample_bytree":** The fraction of features to be used for each tree, suggested as a **float between 0.5 and 1.0**
                    - **"gamma":** The minimum loss reduction required to make a split, suggested as a **float between 0 and 5**
                    - **"lambda" and "alpha":** L1 and L2 regularization parameters, both suggested as **floats between 1e-3 and 10 on a log scale**
                        
                    3.	**Train the Model**
                    For each trial, the model was trained using the **xgb.train** method, with early stopping set to 50 rounds to prevent overfitting.
                        
                    4.	**Optimization Process** 
                    We ran 50 trials to find the optimal hyperparameters, where each trial evaluated a unique combination of parameters. Cross-validation (cv=3) 
                    was used within each trial to ensure that the model's performance was robust and consistent across different splits of the data. The log loss 
                    on the validation set was used as the optimization metric, aiming to minimize it across trials.
                        
                    5.	**Train Final Model with Best Hyperparameters**
                        Once the trials were complete, Optuna provided the combination of hyperparameters that achieved the highest weighted F1 score to train the 
                        final **XGBoost** model. Early stopping was again used, with evaluation metrics reported after every 50 rounds.

                    </div>
                    """,
                    unsafe_allow_html=True)
        st.markdown('<style>[data-testid="stMarkdownContainer"] ul{padding-left:40px;}</style>', unsafe_allow_html = True)
        

        layout_hyperparameter  = st.columns([0.5, 6, 12])
        with layout_hyperparameter[1]:
            st.markdown(f"<h4 style='text-align: center;'>Best Hyperparameters</h4>", unsafe_allow_html = True)

            # Load the JSON file containing the best parameters
            with open("XGBoost Classifier Genre with Optuna/XGBoost_genre_optuna_best_hyperparameters.json", "r") as f:
                best_params_genre = json.load(f)

            df_best_params = pd.DataFrame(best_params_genre.items(), columns=['Parameter', 'Value'])
            st.markdown("""<div style='display: flex; justify-content: center;'><div style='width: 50%;'>""", unsafe_allow_html = True)
            st.dataframe(df_best_params.head(20), height = 350)
            st.markdown("</div></div>", unsafe_allow_html=True)

        with layout_hyperparameter[2]:
            st.markdown(f"<h4 style='text-align: center;'>Hyperparameter Importances</h4>", unsafe_allow_html = True)
            st.image("Visualisations/Hyperparameter Importances.png", use_container_width = True)
        
        with st.expander('Observations on the **Hyperparameter Importances**'):
            st.write("""
                    <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
                     
                    The **Hyperparameter Importance** plot provides a visual representation of how influential each hyperparameter was during the hyperparameter 
                        tuning process conducted using **Optuna**.

                    1.  **Gamma Dominance**  
                    The hyperparameter **gamma** is the most important parameter by a significant margin. It has a value of **nearly 0.9**, indicating that 
                        **regularization strength** (**controlling the gain threshold for splitting nodes**) was crucial in optimizing the 
                        performance of the model. This hyperparameter is often used to **reduce model complexity** by avoiding small, statistically 
                        insignificant splits, effectively making the model less prone to overfitting.
                        
                    2.  **Minimal Influence from Other Parameters**  
                        Other hyperparameters such as **alpha**, **max_depth**, **subsample**, **colsample_bytree**, **learning_rate** and **lambda** have much lower 
                        importances, with values **close to zero**.
                        
                        - The low importance of **alpha** (**L1 regularization** term), **max_depth** (which **controls tree complexity**), and **lambda** (**L2 
                        regularization** term) indicates that the model's performance was not significantly sensitive to these parameters during tuning.
                        
                        - Parameters like **subsample** and **colsample_bytree**, which **control how much data and features are used for tree splits**, also show 
                        little influence, suggesting that the dataset was rich and did not need stringent subsampling to avoid overfitting.
                        
                        - The **learning_rate** parameter also shows minimal importance, which could indicate that the default value chosen in the tuning range was 
                        already sufficient for the data at hand. This suggests that model stability and convergence were likely well-handled by the specified 
                        learning rate range without significant variability.
                    
                    </div>
                    """,
                    unsafe_allow_html=True)
        st.markdown('<style>[data-testid="stMarkdownContainer"] ul{padding-left:40px;}</style>', unsafe_allow_html = True)

    ####################### EVALUATION #######################

    with game_genre_tabs[2]: # 3
        st.header("Model Evaluation") # 4. 
        st.subheader("Accuracy and Log Loss") # 4.1 
        st.write("""
                <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
                 
                To evaluate the performance of the model, we used **Accuracy** and **Log Loss** as our primary metrics. **Accuracy** provides an overall measure of 
                how well the model correctly predicts the target class, offering a straightforward interpretation of performance. Meanwhile, **Log Loss** is particularly 
                well-suited for multiclassification problems as it accounts for the uncertainty of predictions by penalizing incorrect predictions with higher confidence 
                more heavily. Together, these metrics provide a comprehensive understanding of the model's predictive capabilities, balancing both correctness and 
                confidence in its classifications.
                
                </div>
                """,
                unsafe_allow_html=True)
        st.markdown("<div style='margin-top: 10px;'></div>", unsafe_allow_html = True)

        layout_accuracy  = st.columns(3)
        with layout_accuracy[1]:
            st.image("Visualisations/Accuracy & Log Loss.png", use_container_width = False)

        st.write("""
                <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
                 
                - **Accuracy**  
                The final model achieved an accuracy of **0.9412**, indicating that approximately 94% of the predictions were correct. This high accuracy suggests that the 
                model is effectively distinguishing between different game genres.

                - **Log Loss**  
                The log loss of **0.2023** is relatively low, which implies that the predicted probabilities are well-calibrated and the model is confident in its predictions.
                
                </div>
                """,
                unsafe_allow_html=True)
        st.markdown('<style>[data-testid="stMarkdownContainer"] ul{padding-left:40px;}</style>', unsafe_allow_html = True)

        with st.expander("Accuracy by Class"):
            st.image("Visualisations/Accuracy by Class.png", use_container_width = False)

        # ======================================================
        st.subheader("Classification Report") # 4.2 
        layout_clf_report  = st.columns([7.5, 0.5, 5.5, 0.5])
        with layout_clf_report[0]:
            st.write("""
                    <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
                     
                    - **Precision**  
                    Precision measures how many of the predicted genre labels are correct. The precision values across different classes range between **0.81** to **0.96**, 
                    showing that most predictions are highly reliable. Genre **Puzzle (11)** has a slightly lower precision of **0.81**, suggesting that some false 
                    positives exist for this genre.

                    - **Recall**  
                    Recall measures how well the model identifies all instances of each genre. The recall scores range from **0.91** to **0.98**, indicating that the 
                    model successfully identifies most instances of each genre. The lower recall scores for genres such as **Action (0)** indicate that some true instances 
                    were not identified.
                    
                    </div>
                    """,
                    unsafe_allow_html=True)

        with layout_clf_report[2]:
            st.image("Visualisations/Classification Report.png", use_container_width = False)

        with st.expander("F1-Score / Macro Average / Weighted Average / Support"):
            st.write("""
                    <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
                     
                    - **F1-Score**  
                    F1-Score is the harmonic mean of **precision** and **recall**, balancing false positives and false negatives. The F1-scores range between **0.87** and 
                    **0.97**, indicating an overall robust performance across genres. Genres like **Role-Playing (2)**, **Sports (4)** and **Shooter (1)** showed 
                    particularly high F1-scores, indicating that the model is very effective in predicting these genres. However for **Puzzle' (11)** the F1-score is the 
                    lowest at **0.87**, suggesting that this genre may be more challenging for the model to predict, likely due to the **lower support (number of samples)**.

                    - **Macro Average**  
                    The macro average precision, recall, and F1-score are **0.92**, **0.95** and **0.93**, respectively. This indicates that the model is treating each 
                    genre equally well, irrespective of their support. However, the slight difference between precision and recall suggests that certain genres are 
                    predicted with slightly more precision or recall.

                    - **Weighted Average**  
                    The weighted average metrics are all **0.94**, reflecting the overall balance of performance considering the class proportions. This shows that the 
                    model performs consistently across the dataset, giving more weight to the more frequent genres.

                    - **Support**  
                    The support column shows the number of samples for each genre in the test set. Some genres like **Action (4435 samples)** are overrepresented, while 
                    others like **Puzzle (132 samples)** are underrepresented. Despite these differences in support, the model has managed to maintain high performance 
                    for each genre, which is a positive outcome.
                    
                    </div>
                    """,
                    unsafe_allow_html=True)
            st.markdown('<style>[data-testid="stMarkdownContainer"] ul{padding-left:40px;}</style>', unsafe_allow_html = True)

        # ======================================================
        st.subheader("Confusion Matrix") # 4.3 
        layout_cm  = st.columns([7.5, 0.5, 5.5, 0.5])
        with layout_cm[0]:
            st.write("""
                    <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
                     
                    The confusion matrix provides an in-depth view of how the model performed across different classes, allowing us to see the correct and incorrect 
                    predictions for each game genre.

                    The majority of values are concentrated along the diagonal, which means that most predictions are correct. For instance:
                    - **Action (Class 0)** has **4000** correct predictions out of **4435**
                    - **Shooter (Class 1)** has **2615** correct predictions out of **2729**
                    - **Role-Playing (Class 2)** has **2054** correct predictions out of **2126**

                    The dominance along the diagonal line is a strong indicator that the model is effectively distinguishing most genres correctly.
                    
                    </div>
                    """,
                    unsafe_allow_html=True)
            st.markdown('<style>[data-testid="stMarkdownContainer"] ul{padding-left:40px;}</style>', unsafe_allow_html = True)

        with layout_cm[2]:
            st.image("Visualisations/Confusion Matrix.png", use_container_width = False)

        with st.expander('Class-by-Class Analysis'):
            st.write("""
                    <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
                     
                    - **Action (Class 0)**  
                    This genre has **4000** correct predictions. The main misclassifications with 77 and 47 samples were between this class and **Shooter (Class 1)**. 
                    These genres share certain overlapping features, such as fast-paced action or shooting mechanics, which may contribute to confusion during 
                    classification. Further misclassifications were primarily into classes like **Role-Playing (Class 2)** and **Racing (Class 5)**.
                    - **Shooter (Class 1)**  
                    **2615** samples were correctly classified, with minor misclassifications into **Action (Class 0)** and **Sports (Class 4)**. This suggests 
                    that there are similarities in features between the **Action** and **Shooter** genres.
                    - **Role-Playing (Class 2)**  
                    This genre is classified with high accuracy (**2054** correctly classified out of **2126**), indicating the model's strong capability to 
                    recognize the characteristics of Role-Playing games.
                    - **Fighting (Class 7)**  
                    With **596** out of **620** correct predicted samples some misclassifications exist with classes like **Action (Class 0)** and **Shooter (Class 1)**. 
                    Fighting games can sometimes be mistaken for these genres, especially if they have similar elements, such as combat mechanics.
                    - **Adventure (Class 8)**  
                    For this class there were **580** correct predictions out of **601** with misclassifications vs **Strategy (Class 6)**. There are 3 samples from 
                    **Class 8** misclassified as **Class 6**. This suggests some shared elements that can confuse the model, possibly due to similar narrative styles 
                    or player engagement levels.
                    - **Puzzle (Class 11)**  
                    This class had **126** correct predictions out of **132**. Despite having fewer samples, the model was able to classify most of the instances 
                    accurately, showing the effectiveness of oversampling techniques used in training.
                    
                    </div>
                    """,
                    unsafe_allow_html=True)
        st.markdown('<style>[data-testid="stMarkdownContainer"] ul{padding-left:40px;}</style>', unsafe_allow_html = True)

        with st.expander('Performance Highlights'):
            st.write("""
                    <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>

                    - The model performs very well for major genres, such as **Action**, **Shooter** and **Role-Playing**, which have high counts along the diagonal 
                    and minimal off-diagonal values.
                    - The **classes with higher support values** generally have more accurate predictions, reflecting that the model has learned the features of these 
                    well-represented genres better.
                    - There are **minimal false positives** across the confusion matrix, which is a good sign that the model has not overfit to specific genres and 
                    maintains balanced prediction performance.
                    
                    </div>
                    """,
                    unsafe_allow_html=True)
        st.markdown('<style>[data-testid="stMarkdownContainer"] ul{padding-left:40px;}</style>', unsafe_allow_html = True)
        
        # ======================================================
        st.subheader("ROC Curves") # 4.4 
        layout_roc  = st.columns([8, 0.5, 5, 0.5])
        with layout_roc[0]:
            st.write("""
                    <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
                     
                    The AUC scores for all classes are very high, with **AUC = 1.00** for all classes except of **Action (Class 0)** with **AUC = 0.99**. This 
                    indicates that the model has excellent discriminatory power for predicting game genres. Even for minority classes like **Puzzle (Class 11)** and 
                    **Simulation (Class 10)**, which had fewer samples compared to other genres, the model was able to achieve an AUC of **1.00**, indicating that the 
                    oversampling approach was effective in allowing the model to learn these underrepresented classes well.
                    
                    A slightly lower AUC of **0.99** for **Action (Class 0)** compared to the other classes suggests that, while the model performs extremely well for 
                    this genre, it may have some challenges in completely distinguishing it from other similar genres (e.g. **Shooter** or **Fighting**). Given that 
                    **Action** is one of the most common genres in the dataset, it may overlap in features with other genres, which could lead to minor false positives 
                    when predicting.
                    
                    </div>
                    """,
                    unsafe_allow_html=True)
            st.markdown("<div style='margin-top: 10px;'></div>", unsafe_allow_html = True)
        
        with layout_roc[2]:
            st.image("Visualisations/ROC Curves.png", use_container_width = False)
            with st.expander('ROC Plot Explanation'):
                st.write("""
                        <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
                         
                        The **X-axis** of a ROC plot represents the **False Positive Rate (FPR)**, which is the ratio of incorrectly predicted positive instances to 
                        all negative instances. The **Y-axis** represents the **True Positive Rate (TPR)**, which is also known as **recall** or **sensitivity**. The 
                        **AUC (Area Under the Curve)** value is a measure of how well the model distinguishes between classes. An AUC of **1.0** indicates a **perfect 
                        classifier**, while an AUC of **0.5** suggests that the model's performance is equivalent to **random guessing**.  
                        
                        </div>
                        """,
                        unsafe_allow_html=True)

        with st.expander("Implications of the ROC Analysis"):
            st.write("""
                    <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
                     
                    - **Excellent Discriminatory Power**  
                    The high AUC scores indicate that the model has learned the features that differentiate each genre very well, and it is capable of distinguishing 
                    between them with high accuracy.
                    - **Effective Handling of Class Imbalance**  
                    Achieving high AUC for minority classes, such as **Puzzle** and **Simulation**, shows that the **SMOTETomek** oversampling technique was successful 
                    in enabling the model to learn the underrepresented genres effectively.
                    - **Potential Overfitting Concerns**  
                    The near-perfect AUC scores across the board might hint at a potential overfitting issue. This could mean that while the model performs extremely well 
                    on the test set, there might be challenges when deploying the model in real-world scenarios with unseen data. Additional steps, such as cross-validation 
                    or testing on an external dataset, could help further assess the model's robustness.
                    
                    </div>
                    """,
                    unsafe_allow_html=True)
        st.markdown('<style>[data-testid="stMarkdownContainer"] ul{padding-left:40px;}</style>', unsafe_allow_html = True)

        # ======================================================
        st.subheader("Feature Importances") # 4.5 
        layout_features  = st.columns([8, 0.5, 10, 0.5])
        with layout_features[0]:
            st.write("""
                    <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
                     
                    The **Feature Importance plot** reveals the relative influence of different features in predicting the game genre using the XGBoost model. 
                    Understanding which features contributed the most helps in interpreting the model's decisions and identifying key patterns within the data.

                    **Game difficulty** emerged as the most influential feature, highlighting that difficulty levels play a critical role in genre classification. This 
                    finding suggests that genres often exhibit distinct gameplay difficulty profiles that the model successfully captured. Following this, **release year** 
                    ranked as the second most important feature, reflecting the impact of gaming trends and genre prevalence across different periods. **Game completion** 
                    also stood out, indicating that completion rates â€” potentially influenced by gameplay length and style â€” are valuable for distinguishing genres.
                    
                    </div>
                    """,
                    unsafe_allow_html=True)
        st.write("""
                <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
                 
                Moderately influential features **user scores**, **metascores** and **global user scores** offer insights into user experience and the perceived 
                quality of games, which are often tied to genre preferences. Additionally, **publisher** and **platform** were prominent, suggesting that certain 
                publishers and platforms are associated with specific genres due to their production or marketing tendencies.

                Less impactful features, such as **JP sales** and **total sales**, still contributed to genre classification, potentially reflecting regional or 
                genre-specific popularity. Meanwhile, **review_preprocessed** (textual review data) had a role, but quantitative features dominated its predictive 
                power. Features like **play time** and **user review count** were found to have the lowest importance scores, indicating that these metrics are 
                more reflective of user engagement rather than the distinct characteristics of game genres.
                
                </div>
                """,
                unsafe_allow_html=True)
        
        with st.expander("Improvement Areas"):   
            st.write("""
                    <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
                     
                    - **Classes with Similar Features**  
                    Misclassifications between **Action**, **Shooter** and **Fighting** show that these genres might require more refined feature engineering to differentiate 
                    their subtle distinctions.
                    - **Minority Classes**
                    Genres with smaller support counts, such as **Puzzle**, could benefit from additional synthetic data or refined feature extraction to increase model 
                    confidence and reduce misclassifications.
                    
                    </div>
                    """,
                    unsafe_allow_html=True)
        st.markdown('<style>[data-testid="stMarkdownContainer"] ul{padding-left:40px;}</style>', unsafe_allow_html = True)

        with layout_features[2]:
            st.image("Visualisations/Feature Importances.png", use_container_width = False)

if sections == "Game Genre Prediction Demo":

    # Load datasets
    df_reviews = pd.read_csv('all_user_reviews_data_normalized_105000.csv')
    df_games = pd.read_csv('complete_records_3280.csv')
    extract_dir = "temp"  # or another temp subfolder
    df_games_reviews = pd.read_csv(os.path.join(extract_dir, 'all_user_reviews_translated_preprocessed_with_games_data_streamlit.csv'))
    df_games_reviews_X_test = pd.read_csv('all_user_reviews_with_games_data_X_y_test_streamlit.csv')

    st.markdown("<div style='margin-top: 10px;'></div>", unsafe_allow_html = True)  # Adjust spacing before header for "Go to top" button
    st.header("Prediction Demo") # 5. 
    st.subheader("Demo Description")
    
    st.write("""
            <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
             
            The features in this demo are categorized as follows:

            1. **Global Features**  
            These include **platform**, **publisher**, **release year**, **metascore**, **global user score**, **user review count**, **critic review count**, 
            **JP sales**, **total sales** and **average rank**. These features are populated from the test dataset, representing data commonly collected from 
            open sources. They reflect aggregated insights about the game's global reception and performance.

            2. **User Features**  
            Features such as **game difficulty**, **game completion**, **play time**, **user score** and **user review** are also pre-populated from the test dataset. 
            However, these represent user-specific inputs that are typically entered freely by individual users and tailored to their preferences or experiences.

            To ensure usability and practicality, the demo employs a game selection mechanism to pre-fill realistic values for these features. However, the game 
            title itself is anonymized using dummy labels (e.g., *"Game 1"*, *"Game 2"*) to prevent direct linkage between the name and the genre. This approach 
            ensures that the model remains unbiased and does not rely on the game title for predictions.

            The genre prediction is based on diverse and aggregated data inputs, emphasizing a data-driven and impartial methodology. Users can adjust pre-filled 
            feature values manually, providing flexibility while demonstrating how the model leverages comprehensive inputs to deliver accurate genre predictions.
            
            </div>
            """,
            unsafe_allow_html=True)

    # ======================================================
    # Edit Features #
    st.subheader("Edit Game Features")
    st.write("Use populated values for game features and review text or input them manually to predict game genre.")

    # ======================================================
    # Game Dummy #
    df_games_reviews_X_test['game_dummy'] = ['Game ' + str(i) for i in range(1, len(df_games_reviews_X_test) + 1)]

    # Create a mapping dictionary for reference
    title_to_dummy_map = dict(zip(df_games_reviews_X_test['game_title'], df_games_reviews_X_test['game_dummy']))
    dummy_to_title_map = dict(zip(df_games_reviews_X_test['game_dummy'], df_games_reviews_X_test['game_title']))

    # Two Lists with Games Dummies: Either Full Dataset or a Sample Dataset with one game from each genre
    sections = st.radio("Select a dataset:", ["Sample Test Dataset", "Full Test Dataset"], horizontal = True)
    if sections == "Sample Test Dataset":
        selected_game = st.selectbox("**Choose a Game:**", df_games_reviews_X_test.groupby("genre").sample(n = 1, random_state = 42)['game_dummy'])
    else:
        selected_game = st.selectbox("**Choose a Game:**", df_games_reviews_X_test['game_dummy'])

    # Retrieve default values for the selected game
    selected_row = df_games_reviews_X_test[df_games_reviews_X_test["game_dummy"] == selected_game].iloc[0]

    # st.write(selected_row)

    # ======================================================
    # Global Game Features #
    with st.expander("Global Game Features"):
        layout_glob_features_1 = st.columns(3)
        with layout_glob_features_1[0]:
            global_user_score = st.slider("**Global User Score**", 0.0, 10.0, float(selected_row["global_user_score"]), step = 0.1)

        with layout_glob_features_1[1]:
            jp_sales = st.slider("**JP Sales (in millions)**", 0.0, 6.5, float(selected_row["jp_sales"]), step = 0.1)

        with layout_glob_features_1[2]:
            metascore = st.slider("**Metascore**", 20, 100, int(selected_row["metascore"]), step = 1)

        st.markdown("<div style='margin-top: 30px;'></div>", unsafe_allow_html = True)

        layout_glob_features_2 = st.columns(4)
        with layout_glob_features_2[0]:
            total_sales = st.number_input("**Total Sales (in millions)**", value = selected_row["total_sales"], step = 0.1)

        with layout_glob_features_2[1]:
            avg_rank = st.number_input("**Average Rank**", value = selected_row["avg_rank"], step = 1.0)

        with layout_glob_features_2[2]:
            critic_review_count = st.number_input("**Critic Review Count**", value = int(selected_row["critic_review_count"]), step = 1)

        with layout_glob_features_2[3]:
            user_review_count = st.number_input("**User Review Count**", value = int(selected_row["user_review_count"]), step = 1)

        st.markdown("<div style='margin-top: 30px;'></div>", unsafe_allow_html = True)

        layout_glob_features_3 = st.columns(3)
        # Publisher #
        with layout_glob_features_3[0]:
            options = df_games.Publisher.sort_values().unique().tolist()
            publisher = st.selectbox("**Choose a publisher:**", options, index = options.index(selected_row["publisher"]))

        # Platform #
        with layout_glob_features_3[1]:
            platforms_list = df_reviews.platform.sort_values().unique().tolist()
            platform = st.selectbox("**Choose a platform:**", platforms_list, index = platforms_list.index(selected_row["platform"]))

            # Release Year #
        with layout_glob_features_3[2]:
            year_list = df_games.Year.sort_values().unique().tolist()
            release_year = st.selectbox("**Release year:**", year_list, index = year_list.index(int(selected_row["release_year"])))

        st.markdown("<div style='margin-top: 30px;'></div>", unsafe_allow_html = True)

    # ======================================================
    # User Game Features #
    layout_pred_features = st.columns(2)

    # Game Difficulty #
    with layout_pred_features[0]:
        difficulty_ranges = {
                            "Easy": range(10, 20),
                            "Simple": range(20, 30),
                            "Just right": range(30, 40),
                            "Tough": range(40, 50),
                            "Unforgiving": range(50, 60),
                            }

        # Function to map numeric values to labels
        def format_label(value):
            for label, num_range in difficulty_ranges.items():
                if value in num_range:
                    return label

        game_difficulty = st.select_slider(label = "**Select difficulty level:**", options = list(range(10, 60)), value = 35, format_func = format_label)/10

    # Game Completion #
    with layout_pred_features[0]:
        completion_ranges = {
                            "Tried it": range(10, 20),
                            "Played it": range(20, 30),
                            "Halfway": range(30, 40),
                            "Beat it": range(40, 50),
                            "Conquered it": range(50, 60),
                            }

        # Function to map numeric values to labels
        def format_label(value):
            for label, num_range in completion_ranges.items():
                if value in num_range:
                    return label

        game_completion = st.select_slider(label = "**Select completion level:**",
                                            options = list(range(10, 60)),
                                            value = int(selected_row["game_completion"]*10),
                                            format_func = format_label)/10
    
    # Play Time #
    with layout_pred_features[1]:
        play_time = st.slider("**Select play time (in hours):**", 0.5, 80.0, float(selected_row["play_time"]), step = 0.5)

    # User Score #
    with layout_pred_features[1]:
        options = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
        user_score = st.slider("**Choose your score:**", 0, 10, int(selected_row["user_score"]), step = 1)
        # user_score = st.selectbox("**Choose your score:**", options, index = options.index(int(selected_row["user_score"])))

    st.markdown("<div style='margin-top: 30px;'></div>", unsafe_allow_html = True)

    review_text = st.text_area("**Type in your review:**", value = selected_row["review_text"])

    st.markdown("<div style='margin-top: 30px;'></div>", unsafe_allow_html = True)

    X_demo = pd.DataFrame({'platform': [platform],
                            'global_user_score': [global_user_score],
                            'user_score': [user_score],
                            'user_review_count': [user_review_count],
                            'critic_review_count': [critic_review_count],
                            'avg_rank': [avg_rank],
                            'release_year': [release_year],
                            'publisher': [publisher],
                            'jp_sales': [jp_sales],
                            'total_sales': [total_sales],
                            'metascore': [metascore],
                            'play_time': [play_time],
                            'game_difficulty': [game_difficulty],
                            'game_completion': [game_completion],
                            'review_text': [review_text],
                            'review_preprocessed':[np.NaN]})

    # st.dataframe(X_demo)

    # ======================================================
    # Initialize a lemmatizer and Stopwords
    # import nltk
    # from nltk.corpus import stopwords
    # from string import punctuation

    # # Versuche, 'stopwords' zu laden â€“ lade sie bei Fehler herunter
    # try:
    #     stop_words = set(stopwords.words('english')).union(set(punctuation), {"game", "games", 'one'})
    # except LookupError:
    #     nltk.download('stopwords')
    #     stop_words = set(stopwords.words('english')).union(set(punctuation), {"game", "games", 'one'})

    wordnet_lemmatizer = WordNetLemmatizer()
    stop_words = set(stopwords.words('english')).union(set(punctuation), {"game", "games", 'one'})

    # Preprocess text reviews
    def preprocess_text(text):
        text = re.sub(r'\W', ' ', text)  # Substitute everything except of a-z, A-Z, 0-9 and _ with a space
        text = re.sub(r'\s+', ' ', text) # Substitute multiple scpaces with a single space
        text = re.sub(r'\d', ' ', text)  # Substitute all numbers with a space
        text = text.lower()
        text = wordnet_lemmatizer.lemmatize(text)
        text = ' '.join(word for word in text.split() if word not in stop_words)
        return text 

    X_demo['review_preprocessed'] = X_demo.review_text.apply(preprocess_text).astype(str)
    # review_preprocessed = preprocess_text(review_text)

    with st.expander("Test Data"):
        st.write("**X_test Dataframe**")
        st.dataframe(X_demo)

        st.markdown("<div style='margin-top: 30px;'></div>", unsafe_allow_html = True)

        st.write("**Preprocessed Review Text**")
        st.write(X_demo.loc[0, 'review_preprocessed'])

    # ======================================================
    # Load the Best Hyperparameters
    with open("XGBoost Classifier Genre with Optuna/XGBoost_genre_optuna_best_hyperparameters.json", "r") as f:
        best_params_genre = json.load(f)

    # ======================================================
    from xgboost import XGBClassifier

    xgb_model_genre = XGBClassifier(**best_params_genre)
    xgb_model_genre.load_model("XGBoost Classifier Genre with Optuna/XGBoost_genre_optuna_model.json")

    # ======================================================
    # Load Preprocessing Steps
    from joblib import load

    tfidf_genre = load("XGBoost Classifier Genre with Optuna/tfidf_genre.pkl")
    ohe_genre = load("XGBoost Classifier Genre with Optuna/ohe_genre.pkl")
    standard_sc_genre = load("XGBoost Classifier Genre with Optuna/standard_sc_genre.pkl")
    minmax_sc_genre = load("XGBoost Classifier Genre with Optuna/minmax_sc_genre.pkl")
    robust_sc_genre = load("XGBoost Classifier Genre with Optuna/robust_sc_genre.pkl")

    # ======================================================
    # Define lists for categorical and textual feature types
    cat_features = ['platform', 'publisher']
    text_features = 'review_preprocessed'

    # Define separate lists for numerical features according to different scaler applied
    num_features_standard = ['critic_review_count', 'game_difficulty', 'game_completion'] # Features for StandardScaler
    num_features_minmax = ['global_user_score', 'user_score', 'release_year', 'metascore', 'play_time'] # Features for MinMaxScaler
    num_features_robust = ['jp_sales', 'total_sales', 'avg_rank', 'user_review_count'] # Features for RobustScaler

    # ======================================================
    # Transform categorical features
    X_demo_cat = ohe_genre.transform(X_demo[cat_features])
    X_demo_cat = pd.DataFrame(X_demo_cat,
                                columns = ohe_genre.get_feature_names_out(cat_features),
                                index = X_demo.index)

    # Transform text feature
    X_demo_text = tfidf_genre.transform(X_demo[text_features])
    X_demo_text = pd.DataFrame(X_demo_text.toarray(),
                                columns = tfidf_genre.get_feature_names_out(),
                                index = X_demo.index)

    # Transform numerical features
    X_demo_standard = standard_sc_genre.transform(X_demo[num_features_standard])
    X_demo_minmax = minmax_sc_genre.transform(X_demo[num_features_minmax])
    X_demo_robust = robust_sc_genre.transform(X_demo[num_features_robust])

    X_demo_standard = pd.DataFrame(X_demo_standard, columns = num_features_standard, index = X_demo.index)
    X_demo_minmax = pd.DataFrame(X_demo_minmax, columns = num_features_minmax, index = X_demo.index)
    X_demo_robust = pd.DataFrame(X_demo_robust, columns = num_features_robust, index = X_demo.index)

    # Concatenate all preprocessed X_test dataframes together
    X_demo_preprocessed = pd.concat([X_demo_standard,
                                    X_demo_minmax,
                                    X_demo_robust,
                                    X_demo_cat,
                                    X_demo_text], axis = 1)

    st.markdown("<div style='margin-top: 10px;'></div>", unsafe_allow_html = True)

    # ======================================================
    # Predict on preprocessed test data

    st.markdown("""
                <style>
                    div.stButton {
                        display: flex;
                        justify-content: center;
                        margin-top: 20px;
                    }
                    div.stButton > button {
                        font-weight: bold !important;
                        background-color: #5dc762;
                        color: white !important;
                        padding: 10px 20px;
                        border: none;
                        border-radius:
                        5px; cursor: pointer;
                    } 
                    div.stButton > button:hover {
                        background-color: #2e9432;
                    }
                </style>
                """, unsafe_allow_html=True)
    
    if st.button("PREDICT", disabled = False):
        # Run prediction model
        y_pred = xgb_model_genre.predict(X_demo_preprocessed)
        y_pred = list(y_pred)

        genre_mapping = {0: "Action",
                        1: "Shooter",
                        2: "Role-Playing",
                        3: "Platform",
                        4: "Sports",
                        5: "Racing",
                        6: "Strategy",
                        7: "Fighting",
                        8: "Adventure",
                        9: "Misc",
                        10: "Simulation",
                        11: "Puzzle"}

        st.markdown("<div style='margin-top: 10px;'></div>", unsafe_allow_html = True)

        # ======================================================
        # Evaluate the model #
        y_demo = [selected_row["genre_num"]]

        game_title = dummy_to_title_map.get(selected_game)

        # Check if the prediction is correct
        if y_demo == y_pred:
            st.markdown(f"""
                        <div style="display: flex; justify-content: center; text-align: center; font-weight: bold; align-items: center; background-color: #e8f9ee; 
                        color: #177233; padding: 10px; border-radius: 5px; font-size: 25px; width: 100%;">
                            <div>
                                <div>The prediction is correct!</div>
                                <div>This was the game:</div>
                                <div>{game_title.upper()}</div>
                            </div>
                        </div>
                        """, unsafe_allow_html=True)
            
            layout_success = st.columns(3)

            with layout_success[1]:
                st.markdown(f"<h4 style='text-align: center;'>The predicted genre is:</h4>", unsafe_allow_html = True)
                st.markdown(f"<h4 style='text-align: center; font-weight: bold;'>{genre_mapping[y_pred[0]]}</h4>", unsafe_allow_html = True)
                st.image(f"Visualisations/Collages/{genre_mapping[y_pred[0]]}.png", use_container_width = True)

        else:
            st.markdown(f"""
                        <div style="display: flex; justify-content: center; text-align: center; font-weight: bold; align-items: center; background-color: #ffecec;
                        color: #7d353b; padding: 10px; border-radius: 5px; font-size: 25px; width: 100%;">
                            <div>
                                <div>The prediction is wrong!</div>
                                <div>This was the game:</div>
                                <div>{game_title.upper()}</div>
                            </div>
                        </div>
                        """, unsafe_allow_html=True)

            layout_failure = st.columns([1,4,1,4,1])

            with layout_failure[1]:
                st.markdown(f"<h4 style='text-align: center;'>The predicted genre is:</h4>", unsafe_allow_html = True)
                st.markdown(f"<h4 style='text-align: center; font-weight: bold;'>{genre_mapping[y_pred[0]]}</h4>", unsafe_allow_html = True)
                st.image(f"Visualisations/Collages/{genre_mapping[y_pred[0]]}.png", use_container_width = True)
            
            with layout_failure[3]:
                st.markdown(f"<h4 style='text-align: center;'>The actual genre is:</h4>", unsafe_allow_html = True)
                st.markdown(f"<h4 style='text-align: center;font-weight: bold;'>{genre_mapping[y_demo[0]]}</h4>", unsafe_allow_html = True)
                st.image(f"Visualisations/Collages/{genre_mapping[y_demo[0]]}.png", use_container_width = True)

        st.markdown("<div style='margin-top: 40px;'></div>", unsafe_allow_html = True)

        with st.expander("Model Evaluation"):
            # ======================================================
            # Class Predicted Probabilities
            st.write("**Class Predicted Probabilities**")

            y_pred_proba = xgb_model_genre.predict_proba(X_demo_preprocessed)

            # Convert probabilities to a DataFrame for display
            df_pred_proba = pd.DataFrame(y_pred_proba, columns = [f"{genre_mapping[i]}" for i in range(y_pred_proba.shape[1])])

            # Use Pandas Styler to apply a color gradient
            st.dataframe(df_pred_proba.style.background_gradient(cmap = 'Blues', axis = 1))

            st.markdown("<div style='margin-top: 10px;'></div>", unsafe_allow_html = True)

            # ======================================================
            # Log Loss #
            st.write("**Log Loss**")

            from sklearn.metrics import log_loss

            num_classes = 12
            labels = list(range(num_classes))  # All possible class labels (0 to 11)
            logloss = log_loss(y_demo, y_pred_proba, labels = labels)
            st.write(f"Model Log Loss: {logloss:.4f}")

            st.markdown("<div style='margin-top: 10px;'></div>", unsafe_allow_html = True)

            # ======================================================
            # # Classification report #
            # st.write("**Classification Report**")

            # from sklearn.metrics import classification_report

            # if y_demo == y_pred:
            #     target_name = [genre_mapping[y_pred[0]]]
            # else:
            #     target_name = [genre_mapping[y_pred[0]]] + [genre_mapping[y_demo[0]]]

            # cl_report_dict = classification_report(y_demo, y_pred, target_names = target_name, output_dict = True)

            # # Convert the dictionary to a DataFrame for better visualization
            # df_cl_report = pd.DataFrame(cl_report_dict).transpose()
            # df_cl_report = df_cl_report.round(2)

            # st.dataframe(df_cl_report.style.format("{:.2f}").highlight_max(axis = 0, color = "lightgreen"))

            # st.markdown("<div style='margin-top: 10px;'></div>", unsafe_allow_html = True)

            # if y_demo != y_pred:
            #     # ======================================================
            #     # # Confusion Matrix #
            #     st.write("**Confusion Matrix**")

            #     layout_cm = st.columns(3)

            #     with layout_cm[0]:
            #         from sklearn.metrics import confusion_matrix

            #         # Confusion Matrix
            #         conf_matr = confusion_matrix(y_demo, y_pred)

            #         fig, ax = plt.subplots(figsize = (5, 5))
            #         sns.heatmap(conf_matr, annot = True, fmt = '.0f', cmap = 'Blues')

            #         ax.set_xlabel('Predicted Class', fontsize = 12)
            #         ax.set_ylabel('Actual Class', fontsize = 12)
            #         ax.set_title('Confusion Matrix', fontsize = 14)
            #         ax.tick_params(axis = 'both', labelsize = 10)

            #         st.pyplot(fig)

            #         st.markdown("<div style='margin-top: 10px;'></div>", unsafe_allow_html = True)

            #     # ======================================================
            #     # # ROC Curve #
            #     st.write("**ROC Curve**")

            #     from sklearn.metrics import roc_curve, auc

            #     # fpr, tpr, _ = roc_curve((y_demo == 1).astype(int), y_pred_proba[:, y_demo[0]])

            #     fpr, tpr, _ = roc_curve([1 if y_demo[0] == i else 0 for i in range(len(y_pred_proba[0]))], y_pred_proba[0])

            #     layout_roc = st.columns(3)

            #     with layout_roc[0]:
            #         fig, ax = plt.subplots(figsize=(5, 5))
            #         ax.plot(fpr, tpr, label = f"Class {genre_mapping[y_demo[0]]} (AUC = {auc(fpr, tpr):.2f})")
            #         ax.plot([0, 1], [0, 1], color = 'gray', linestyle='--', label = "Random Classifier (AUC = 0.50)")

            #         ax.set_xlabel("False Positive Rate")
            #         ax.set_ylabel("True Positive Rate")
            #         ax.set_title(f"ROC Curve for {genre_mapping[y_demo[0]]} (Class {y_demo[0]})")
            #         ax.legend(loc = "lower right")

            #         st.pyplot(fig)

    st.markdown('<style>div[class*="stTextArea"] label p {font-size: 18px;}</style>', unsafe_allow_html = True)
    st.markdown('<style>div[class*="stSelectbox"] label p {font-size: 18px; height: 40px;}</style>', unsafe_allow_html = True)
    st.markdown('<style>div[class*="stMultiSelect"] label p {font-size: 18px;}</style>', unsafe_allow_html = True)
    st.markdown('<style>div[class*="stSlider"] label p {font-size: 18px;}</style>', unsafe_allow_html = True)
    st.markdown('<style>div[class*="stNumberInput"] label p {font-size: 18px;}</style>', unsafe_allow_html = True)

if sections == "Project Conclusion":
    st.markdown("<div style='margin-top: 10px;'></div>", unsafe_allow_html = True)  # Adjust spacing before header for "Go to top" button
    st.header("Project Conclusion")
    st.subheader("Challanges")
    st.write("""
            <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
             
            Throughout this project, several obstacles were encountered that required strategic adjustments:  

            1.	**Data Acquisition and Imbalance**
            The dataset was not only large, but also highly imbalanced, with a few genres (e.g. **'Action'** or **'Shooter'**) significantly overrepresented. This 
            required the implementation of **SMOTETomek** to ensure balanced learning, which added complexity and processing time.
            2.	**Computational Resources**  
            The large dataset presented computational challenges, particularly when applying resampling techniques like **SMOTETomek**. To manage this, we employed 
            several data reduction techniques, including splitting and reducing training sets.
            3.	**Feature Engineering**  
            Renaming columns (**release_year**, **game_difficulty** and **game_completion**) to avoid conflicts with vectorized features, recalculating of games 
            metrics and analysing of highly correlated features to reduce the total number of features added complexity to the data preparation process.
            
            Despite these challenges, the approach and model showed promising results, and the use of an XGBoost classifier with hyperparameter tuning allowed us 
            to achieve significant predictive performance.
            
            </div>
            """,
            unsafe_allow_html=True)
    
    st.subheader("Future Improvements")
    st.write("""
            <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
             
            1.	**Improving Model Performance**  
            Further performance gains could be achieved by experimenting with different model architectures, such as deep learning approaches (e.g., recurrent 
            neural networks for text reviews). Additionally, optimizing hyperparameters with more trials in Optuna or considering ensemble methods could enhance 
            results.
            2.	**Feature Engineering**  
            More advanced feature engineering, such as deriving additional temporal features (e.g., release quarter or trend analysis of genre popularity), could 
            potentially improve model performance. Additionally, sentiment analysis on user reviews could provide deeper insights and better features for genre 
            classification.
            3.	**Handling Data Size and Computation**  
            A cloud-based approach or GPU-based processing could help handle larger datasets more effectively, especially for resampling and training phases. 
            This would alleviate the computational constraints faced in this project.
            4.	**Application of Findings**  
            The insights gained from feature importance and model performance could be applied in game recommendation systems or for targeted marketing, where 
            predicting a userâ€™s preferred genre accurately can boost user engagement and sales. Additionally, publishers could leverage this model to predict 
            potential genres for new game concepts based on historical features.
            
            </div>
            """,
            unsafe_allow_html=True)
    
    st.subheader("Feature Importances")
    st.write("""
            <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
             
            The feature importances suggest that the model's performance and predictions rely more heavily on quantitative and categorical features, such as 
            **game_difficulty**, **release_year** and **game_completion**, rather than textual data derived from user reviews (**review_preprocessed**). 

            Given the goal of predicting the game genre primarily based on user reviews, the relatively low importance of **review_preprocessed** indicates that 
            the textual data extracted from reviews is not as influential in determining the game's genre as expected. This may be due to several factors:

            1. **Quality of Preprocessed Reviews**  
            Despite preprocessing (e.g., removing stop words, lemmatization), the signal within user reviews may not strongly correlate with genre distinctions. 
            Users may discuss gameplay, story, or mechanics in ways that overlap across genres, making it challenging for the model to differentiate based on 
            reviews alone.

            2. **Strength of Quantitative Features**  
            Features like **game_difficulty**, **release_year** and **game_completion** inherently carry structured and genre-specific information that the model 
            can more easily utilize for prediction. For example, the difficulty and completion rates likely vary significantly between genres, providing clear 
            signals for classification.

            3. **Representation of Review Data**  
            The way **review_preprocessed** was represented (e.g., TF-IDF) may not fully capture the nuanced differences between genres. Alternative approaches 
            like word embeddings (e.g., Word2Vec, GloVe) or advanced contextual models (e.g., BERT) might better capture these nuances.
            
            </div>
            """,
            unsafe_allow_html=True)
    
    st.subheader("Implications")
    st.write("""
            <div style='text-align: justify; font-size: 16px; line-height: 1.6;'>
             
            - **Improving Textual Data's Contribution**  
            To make reviews a more dominant factor in genre prediction, consider enhancing the representation of textual data by using embeddings or domain-specific 
            sentiment and semantic analyses. Additionally, reducing reliance on quantitative features or retraining the model with only textual data could shift 
            the balance.

            - **Complementary Use of Features**  
            The importance of structured data suggests that textual data alone may not be sufficient for genre prediction. Instead, user reviews might serve as a 
            complementary feature alongside other quantitative data.

            In summary, while the model leverages structured features effectively, the relatively low importance of **review_preprocessed** highlights the 
            challenge of extracting genre-specific insights from user reviews alone. Future adjustments in preprocessing or representation methods may help 
            increase the predictive power of textual data.
            
            </div>
            """,
            unsafe_allow_html=True)
     
    st.markdown('<style>[data-testid="stMarkdownContainer"] ul{padding-left:40px;}</style>', unsafe_allow_html = True)
    st.markdown("<div style='margin-top: 30px;'></div>", unsafe_allow_html = True) 

    # Create a layout with columns to center the content
    layout_prinzess_gif = st.columns([1, 3, 1]) 

    with layout_prinzess_gif[1]:
        st.markdown(f"<h3 style='text-align: center; font-weight: bold;'>Thank you!</h3>", unsafe_allow_html = True)
        st.image('Visualisations/Streamlit Pictures/mario-princess.gif', width=600)

if sections == "Credits":
    st.markdown("<div style='margin-top: 10px;'></div>", unsafe_allow_html = True)  # Adjust spacing before header for "Go to top" button
    st.header("Credits")
    
    st.write("") # Kleiner Abstand oben

    layout_credits_1 = st.columns([1, 4])  # Adjust the column widths for centering

    with layout_credits_1[0]:
        st.markdown("""
                    <a href="https://datascientest.com/" target="_blank">
                        <img src="https://datascientest.com/wp-content/uploads/2022/03/logo-2021.png" alt="DataScientest Logo" width="150">
                    </a>
                    """, unsafe_allow_html=True)
        
        st.markdown("") # Kleiner Abstand oben

    with layout_credits_1[1]:
        st.write("""
            This project was carried out as part of a Data Analytics training course at the [DataScientest](https://datascientest.com) institute
            """)
    
    st.write("") # Kleiner Abstand oben

    layout_credits_1 = st.columns([1, 4])  # Adjust the column widths for centering

    with layout_credits_1[0]:
        st.markdown("Project carried out by:")

    with layout_credits_1[1]:
        st.markdown("""
                    <style>
                    .image-link-container {
                        text-decoration: none;
                        color: #262730;
                        display: inline-flex;
                        align-items: center;
                        gap: 5px;
                    }
                    .image-link-container img {
                        height: 1em; /* Passt die BildhÃ¶he an die SchriftgrÃ¶ÃŸe an */
                        width: auto;
                        vertical-align: middle;
                    }
                    </style>
                        
                    <a href="https://www.linkedin.com/in/vyacheslavgoncharenko/" class="image-link-container">
                        Vyacheslav Goncharenko
                        <img src="https://www.ionos.at/digitalguide/fileadmin/_processed_/e/1/csm_screenshot-des-favicons-von-linkedin_f17513dfd6.webp" alt="LinkedIn Icon" />
                    </a>
                    """,
                    unsafe_allow_html=True)
    
    layout_credits_2 = st.columns([1, 4])  # Adjust the column widths for centering

    with layout_credits_2[0]:
        st.markdown("Project mentor:")
        
    with layout_credits_2[1]:
        st.markdown("""
                    <style>
                    .image-link-container {
                        text-decoration: none;
                        color: #262730;
                        display: inline-flex;
                        align-items: center;
                        gap: 5px;
                    }
                    .image-link-container img {
                        height: 1em; /* Passt die BildhÃ¶he an die SchriftgrÃ¶ÃŸe an */
                        width: auto;
                        vertical-align: middle;
                    }
                    </style>
                        
                    <a href="https://www.linkedin.com/in/yaniv-benichou/" class="image-link-container">
                        Yaniv Benichou 
                        <img src="https://www.ionos.at/digitalguide/fileadmin/_processed_/e/1/csm_screenshot-des-favicons-von-linkedin_f17513dfd6.webp" alt="LinkedIn Icon" />
                    </a>
                    """,
                    unsafe_allow_html=True)
    
    layout_credits_2 = st.columns([1, 4])  # Adjust the column widths for centering

    with layout_credits_2[0]:
        st.markdown("Data provided by:")

    with layout_credits_2[1]:
        st.markdown("""
                    <style>
                    .image-link-container {
                        text-decoration: none;
                        color: #262730;
                        display: inline-flex;
                        align-items: center;
                        gap: 5px;
                    }
                    .image-link-container img {
                        height: 1em; /* Passt die BildhÃ¶he an die SchriftgrÃ¶ÃŸe an */
                        width: auto;
                        vertical-align: middle;
                    }
                    </style>

                    <a href="https://www.linkedin.com/in/aaron-martin-phd/" class="image-link-container">
                        Aaron Martin, Ph.D. 
                        <img src="https://www.ionos.at/digitalguide/fileadmin/_processed_/e/1/csm_screenshot-des-favicons-von-linkedin_f17513dfd6.webp" alt="LinkedIn Icon" />
                    </a><br>

                    (<a href = "https://www.kaggle.com/datasets/aaronrmartin/video-games-gameplay-reviews-sentiment-scores" target="_blank">https://www.kaggle.com/datasets/aaronrmartin/video-games-gameplay-reviews-sentiment-scores</a>)
                    """,
                    unsafe_allow_html=True)

    # Create a layout with columns to center the content
    layout_coin_gif = st.columns([1, 1, 1]) 

    with layout_coin_gif[1]:
        st.image('Visualisations/Streamlit Pictures/coin-mario.gif', width=200)
